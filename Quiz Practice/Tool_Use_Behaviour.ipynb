{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "knVJO4Ox_6U3"
      ],
      "authorship_tag": "ABX9TyNALp3HnAfkmzKZfOHAMlyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyg1/Q4/blob/main/Quiz%20Practice/Tool_Use_Behaviour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gedVA8MfqUeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "f2c60adb-8c6b-4f62-b86d-f3c1e16e7533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ye8rjqg1tyAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, OpenAIChatCompletionsModel, set_tracing_export_api_key, trace\n",
        "from openai import AsyncOpenAI\n",
        "from google.colab  import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "set_tracing_export_api_key(OPENAI_API_KEY)\n",
        "\n",
        "Client = AsyncOpenAI(\n",
        "    api_key = GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    openai_client= Client\n",
        ")                                                                      #Setting up the environment"
      ],
      "metadata": {
        "id": "pmZndHbAu7OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Synchronus & Asynchronus Behaviour"
      ],
      "metadata": {
        "id": "knVJO4Ox_6U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def get_weather(location: str) -> str:          #Async Runner & Agent with sync function as tool\n",
        "  return f\"The weather in {location} is Sunny\"\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDimZs3NznfP",
        "outputId": "f303b03e-e51b-402e-f78d-a5996e3c470a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def get_weather(location: str) -> str:\n",
        "  return f\"The weather in {location} is Sunny\"\n",
        "\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        " )\n",
        "\n",
        "async def main():\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)           #Async Runner  with synchronus Agent & function  as a tool\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmHWWpLZ8_YJ",
        "outputId": "bc50ec53-23ae-47fd-b16d-2f3998028602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:          #Async Runner & Agent & function as tool\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2sc-xhv9787",
        "outputId": "7cc91d4b-f637-4500-8c1f-571d90842d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:          #Async  Agent & function as tool with sync runner\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())\n",
        "\n",
        "with trace(\"Tool Use\"):\n",
        " res =  Runner.run_sync(agent,\"What's weather like in Karachi?\")\n",
        " print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8-6cW1eHABsY",
        "outputId": "8232f2f7-17a1-4108-b7b0-62b563e76982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Assistant\", ...)\n",
            "- Final output (str):\n",
            "    The weather in Karanchi is Sunny.\n",
            "- 3 new item(s)\n",
            "- 2 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tool Use Behaviour"
      ],
      "metadata": {
        "id": "0eSqTeE7KitY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "  )                                              # tool_choice = auto is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = none or empty will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"Hey there!\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuHo50gXuQim",
        "outputId": "fc8f66bd-e544-4a04-a514-e937ec68a348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sunny day greets,\n",
            "New York's clear skies, a bright start,\n",
            "Nature's gentle smile.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      tool_use_behavior= \"stop_on_first_tool\",\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "    #   In traces we find out that because of tool_use_behavior= \"stop_on_first_tool\" the\n",
        "    #   agent loop stops and original output of the tool is saved as final_output.\n",
        "    #   by default it is \"run_llm_again\"\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Agf-rKLexF",
        "outputId": "4c06725f-0ff4-4983-b038-05bee09526ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "from agents.agent import StopAtTools\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  @function_tool\n",
        "  async def  get_news(location: str) -> str:\n",
        "   return  f\"Only Good news are coming from {location}\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather, get_news],\n",
        "      tool_use_behavior= StopAtTools(stop_at_tool_names=[\"get_weather\"])\n",
        "  )\n",
        "#   Stop at Tool use but for specific tool from multiple tools\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What are news like in Karachi & how's weather like?\")\n",
        "    print(res.final_output)\n",
        "#  LLM calls both tools but since it's instructed to stop at \"get_weather\", it stops and returns the\n",
        "#  \"get_weather\" output to final_output\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQHrZ86-lByn",
        "outputId": "ac4f7bed-00f0-4136-cb39-67c890840712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karachi is Sunny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "      reset_tool_choice=True, # this is prefered now and tool choice required is now not implemented\n",
        "\n",
        "  )                                              # tool_choice = auto is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = \"none\" /None or empty will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"Hey there!\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjOR8P24w0ZC",
        "outputId": "3dabde55-0e43-436f-e1c2-22d7515d28a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good day to you!\n",
            "How may I help you today?\n",
            "Tell me your desires.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "  )\n",
        "  #  Max turns exceeded error will be thrown\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's the weather like in Karachi?\",max_turns=1)\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "937yC6ERyQ1M",
        "outputId": "54ff6f8a-5cf8-4b80-be1c-73ae0ce69cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MaxTurnsExceeded",
          "evalue": "Max turns (1) exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-51-4045774385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-51-4045774385.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#  Max turns exceeded error will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tool Use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"What's the weather like in Karachi?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         return await runner.run(\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                             ),\n\u001b[1;32m    404\u001b[0m                         )\n\u001b[0;32m--> 405\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mMaxTurnsExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max turns ({max_turns}) exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     logger.debug(\n",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m: Max turns (1) exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "      reset_tool_choice=False, # this is prefered now and tool choice reset will bring us into tool calling loop\n",
        "  )        # if max_turns would not have been applied it will continue calling tools and then llm until api rate limit or perhaps default max_turns = 10\n",
        "                                         #  Max turns exceeded error will be thrown\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"Hi\",max_turns=3)\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "_EEdBIQm0OsE",
        "outputId": "4f872b1e-b276-45ca-d034-a1e0cbe6247d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MaxTurnsExceeded",
          "evalue": "Max turns (3) exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52-774086153.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-52-774086153.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#  Max turns exceeded error will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tool Use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         return await runner.run(\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                             ),\n\u001b[1;32m    404\u001b[0m                         )\n\u001b[0;32m--> 405\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mMaxTurnsExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max turns ({max_turns}) exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     logger.debug(\n",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m: Max turns (3) exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\".45.,/j[\"),\n",
        "      reset_tool_choice=False,\n",
        "\n",
        "  )                                              # tool_choice = auto/None is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = \"none\" or empty or any string value will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_8hqNVj2l9j",
        "outputId": "9bc0cead-4cb6-4d09-915a-984307966540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karachi's weather\n",
            "Sunny skies, warm, gentle breeze\n",
            "A pleasant day calls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parallel Tool Call"
      ],
      "metadata": {
        "id": "B2U3TxJS5fcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  @function_tool\n",
        "  async def  get_news(location: str) -> str:\n",
        "   return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok since the whole nb is Async the parallel_tool_calls=False is not stopping parallel tool call\n",
        "  with trace(\"Tool Use\"):            # we will try again in next cell with synchronus stuff\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CGFxZH35jJi",
        "outputId": "0c228dac-e2c4-4d94-c8b0-18038f2f4de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sunny skies appear,\n",
            "Good news from Karachi comes,\n",
            "Hope fills the warm air.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "@function_tool\n",
        "def  get_weather(location: str) -> str:\n",
        " return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "@function_tool\n",
        "def  get_news(location: str) -> str:\n",
        " return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "async def main():\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok now since the tools are sync and rest of nb is Async the parallel_tool_calls=False is not stopping parallel tool call\n",
        "  with trace(\"Tool Use\"):           # try again next cell\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTMwPgbS86b1",
        "outputId": "d7897e87-4a37-435d-a265-4f8feb9ba8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun shines in Karachi,\n",
            "Good news brings a happy day,\n",
            "Peace fills the warm air.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "@function_tool\n",
        "def  get_weather(location: str) -> str:\n",
        " return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "@function_tool\n",
        "def  get_news(location: str) -> str:\n",
        " return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok now since whole nb is Async the parallel_tool_calls=False is still not stopping parallel tool call\n",
        "with trace(\"Tool Use\"):\n",
        "    res =  Runner.run_sync(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiBr-5MZ9o8B",
        "outputId": "3998de1a-8b97-4c66-8548-212da96d2b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather is sunny\n",
            "Good news is coming from there\n",
            "Karachi is bright\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dynamic Instructions - Context Engineering"
      ],
      "metadata": {
        "id": "5OgSC5Q_PMXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_instructions(context, agent) -> str:\n",
        "  print(f\"context: {context}\")\n",
        "  print(f\"\\nagent: {agent}\\n\")\n",
        "  return f\"Respond in haiku\"\n",
        "                                # without typing\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = dynamic_instructions,\n",
        "      model = model,\n",
        "  )\n",
        "\n",
        "with trace(\"Dynamic Instructions\"):\n",
        "    res =  Runner.run_sync(agent,\"Yo ðŸ¥€ \")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjCeNrtvPesB",
        "outputId": "0d698484-136f-47f6-be04-98a6d36445bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "\n",
            "agent: Agent(name='Assistant', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=<function dynamic_instructions at 0x785ff0ef11c0>, prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "\n",
            "Crimson petals fall,\n",
            "A gentle, whispered goodbye,\n",
            "Beauty fades with time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import RunContextWrapper\n",
        "\n",
        "def dynamic_instructions(ctx: RunContextWrapper, starting_agent: Agent) -> str:\n",
        "  print(f\"context: {ctx}\")\n",
        "  print(f\"\\ncontext-context: {ctx.context}\")\n",
        "  print(f\"\\nagent: {starting_agent}\")\n",
        "  print(f\"\\nagent-model: {starting_agent.model}\\n\")\n",
        "  return f\"Respond in haiku\"\n",
        "                                                       # with typing\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = dynamic_instructions,\n",
        "      model = model,\n",
        "  )\n",
        "\n",
        "with trace(\"Dynamic Instructions\"):\n",
        "    res =  Runner.run_sync(agent,\"Yo ðŸ¥€ \")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qqwjk8dWCWg",
        "outputId": "1d8c0c16-1aa6-44db-f451-aaeacd2e0202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "\n",
            "context-context: None\n",
            "\n",
            "agent: Agent(name='Assistant', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=<function dynamic_instructions at 0x785ff0ef1da0>, prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "\n",
            "agent-model: <agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>\n",
            "\n",
            "Crimson petals fall,\n",
            "A silent, gentle goodbye,\n",
            "Beauty's brief farewell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_instructions(agent, context) -> str:\n",
        "  print(f\"context: {context}\") # agent is stored as context\n",
        "  print(f\"\\nagent: {agent}\\n\")# context is stored as agent\n",
        "  return f\"Respond in haiku\"\n",
        "                                # without typing parameter swap\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = dynamic_instructions,\n",
        "      model = model,\n",
        "  )\n",
        "\n",
        "with trace(\"Dynamic Instructions\"):\n",
        "    res =  Runner.run_sync(agent,\"Yo ðŸ¥€ \")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnVDpqqhalE-",
        "outputId": "e6a57aff-5c96-4d58-c241-1962da956342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: Agent(name='Assistant', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=<function dynamic_instructions at 0x785ff0ef19e0>, prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "\n",
            "agent: RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "\n",
            "Petals softly fall,\n",
            "A fleeting bloom, then gone,\n",
            "Summer's gentle sigh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import RunContextWrapper\n",
        "\n",
        "def dynamic_instructions(starting_agent: Agent, ctx: RunContextWrapper) -> str:\n",
        "  print(f\"context: {ctx}\")\n",
        "  print(f\"\\ncontext-context: {ctx.context}\")\n",
        "  print(f\"\\nagent: {starting_agent}\")\n",
        "  print(f\"\\nagent-model: {starting_agent.model}\\n\")\n",
        "  return f\"Respond in haiku\"\n",
        "                                                       # with typing parameter swap ctx doesnt have context property the values swaped that's why error\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = dynamic_instructions,\n",
        "      model = model,\n",
        "  )\n",
        "\n",
        "with trace(\"Dynamic Instructions\"):\n",
        "    res =  Runner.run_sync(agent,\"Yo ðŸ¥€ \")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "VW1W_6Cea4OV",
        "outputId": "634dc1d8-6610-4367-98c8-df2e42c8a71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: Agent(name='Assistant', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=<function dynamic_instructions at 0x785ff0ef1120>, prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Agent' object has no attribute 'context'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-3442290592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dynamic Instructions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Yo ðŸ¥€ \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \"\"\"\n\u001b[1;32m    259\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         return runner.run_sync(\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"session\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         return asyncio.get_event_loop().run_until_complete(\n\u001b[0m\u001b[1;32m    514\u001b[0m             self.run(\n\u001b[1;32m    515\u001b[0m                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                         input_guardrail_results, turn_result = await asyncio.gather(\n\u001b[0m\u001b[1;32m    413\u001b[0m                             self._run_input_guardrails(\n\u001b[1;32m    414\u001b[0m                                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_run_single_turn\u001b[0;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[1;32m    948\u001b[0m             )\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         system_prompt, prompt_config = await asyncio.gather(\n\u001b[0m\u001b[1;32m    951\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_system_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/agent.py\u001b[0m in \u001b[0;36mget_system_prompt\u001b[0;34m(self, run_context)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAwaitable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstructions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Instructions must be a string or a function, got {self.instructions}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-31-3442290592.py\u001b[0m in \u001b[0;36mdynamic_instructions\u001b[0;34m(starting_agent, ctx)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdynamic_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_agent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunContextWrapper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"context: {ctx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\ncontext-context: {ctx.context}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nagent: {starting_agent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nagent-model: {starting_agent.model}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'context'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import RunContextWrapper\n",
        "\n",
        "def dynamic_instructions(ctx: RunContextWrapper, starting_agent: Agent) -> str:\n",
        "  print(f\"context: {ctx}\")\n",
        "  print(f\"\\ncontext-context: {ctx.context}\")\n",
        "  print(f\"\\nagent: {starting_agent}\")\n",
        "  print(f\"\\nagent-instructions: {starting_agent.instructions}\\n\")\n",
        "  return f\"Respond in haiku\"\n",
        "                                                       # context a python object passed in runner\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = dynamic_instructions,\n",
        "      model = model,\n",
        "  )\n",
        "\n",
        "with trace(\"Dynamic Instructions\"):\n",
        "    res =  Runner.run_sync(agent,\"Yo ðŸ¥€ \",context=\"Ali\")\n",
        "    print(res.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s31VHYU1hCFO",
        "outputId": "13099eb6-9638-435d-ca3b-a7c6d5ad2f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: RunContextWrapper(context='Ali', usage=Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0))\n",
            "\n",
            "context-context: Ali\n",
            "\n",
            "agent: Agent(name='Assistant', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=<function dynamic_instructions at 0x785ff0f43b00>, prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x785ffd312210>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
            "\n",
            "agent-instructions: <function dynamic_instructions at 0x785ff0f43b00>\n",
            "\n",
            "Dark bloom softly falls,\n",
            "A whisper in the night wind,\n",
            "Fading, then reborn.\n"
          ]
        }
      ]
    }
  ]
}