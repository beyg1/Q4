{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "knVJO4Ox_6U3"
      ],
      "authorship_tag": "ABX9TyNvonJZ5lAjtv0tPxSxMA5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyg1/Q4/blob/main/Quiz%20Practice/Tool_Use_Behaviour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gedVA8MfqUeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e6a01027-6de7-4ff2-c4d6-ef0478f4e43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ye8rjqg1tyAY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, OpenAIChatCompletionsModel, set_tracing_export_api_key, trace\n",
        "from openai import AsyncOpenAI\n",
        "from google.colab  import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "set_tracing_export_api_key(OPENAI_API_KEY)\n",
        "\n",
        "Client = AsyncOpenAI(\n",
        "    api_key = GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    openai_client= Client\n",
        ")                                                                      #Setting up the environment"
      ],
      "metadata": {
        "id": "pmZndHbAu7OS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Synchronus & Asynchronus Behaviour"
      ],
      "metadata": {
        "id": "knVJO4Ox_6U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def get_weather(location: str) -> str:          #Async Runner & Agent with sync function as tool\n",
        "  return f\"The weather in {location} is Sunny\"\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDimZs3NznfP",
        "outputId": "f303b03e-e51b-402e-f78d-a5996e3c470a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def get_weather(location: str) -> str:\n",
        "  return f\"The weather in {location} is Sunny\"\n",
        "\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        " )\n",
        "\n",
        "async def main():\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)           #Async Runner  with synchronus Agent & function  as a tool\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmHWWpLZ8_YJ",
        "outputId": "bc50ec53-23ae-47fd-b16d-2f3998028602"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:          #Async Runner & Agent & function as tool\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2sc-xhv9787",
        "outputId": "7cc91d4b-f637-4500-8c1f-571d90842d8d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:          #Async  Agent & function as tool with sync runner\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"Concise Assistant\",\n",
        "      model = model,\n",
        "      tools = [get_weather]\n",
        "  )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())\n",
        "\n",
        "with trace(\"Tool Use\"):\n",
        " res =  Runner.run_sync(agent,\"What's weather like in Karachi?\")\n",
        " print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8-6cW1eHABsY",
        "outputId": "8232f2f7-17a1-4108-b7b0-62b563e76982"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"Assistant\", ...)\n",
            "- Final output (str):\n",
            "    The weather in Karanchi is Sunny.\n",
            "- 3 new item(s)\n",
            "- 2 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tool Use Behaviour"
      ],
      "metadata": {
        "id": "0eSqTeE7KitY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "  )                                              # tool_choice = auto is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = none or empty will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"Hey there!\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuHo50gXuQim",
        "outputId": "fc8f66bd-e544-4a04-a514-e937ec68a348"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sunny day greets,\n",
            "New York's clear skies, a bright start,\n",
            "Nature's gentle smile.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      tool_use_behavior= \"stop_on_first_tool\",\n",
        "  )\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's weather like in Karachi?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "    #   In traces we find out that because of tool_use_behavior= \"stop_on_first_tool\" the\n",
        "    #   agent loop stops and original output of the tool is saved as final_output.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Agf-rKLexF",
        "outputId": "4c06725f-0ff4-4983-b038-05bee09526ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karanchi is Sunny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool\n",
        "from agents.agent import StopAtTools\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  @function_tool\n",
        "  async def  get_news(location: str) -> str:\n",
        "   return  f\"Only Good news are coming from {location}\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather, get_news],\n",
        "      tool_use_behavior= StopAtTools(stop_at_tool_names=[\"get_weather\"])\n",
        "  )\n",
        "#   Stop at Tool use but for specific tool from multiple tools\n",
        "\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What are news like in Karachi & how's weather like?\")\n",
        "    print(res.final_output)\n",
        "#  LLM calls both tools but since it's instructed to stop at \"get_weather\", it stops and returns the\n",
        "#  \"get_weather\" output to final_output\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQHrZ86-lByn",
        "outputId": "ac4f7bed-00f0-4136-cb39-67c890840712"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Karachi is Sunny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "      reset_tool_choice=True, # this is prefered now and tool choice required is now not implemented\n",
        "\n",
        "  )                                              # tool_choice = auto is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = \"none\" /None or empty will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"Hey there!\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjOR8P24w0ZC",
        "outputId": "3dabde55-0e43-436f-e1c2-22d7515d28a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good day to you!\n",
            "How may I help you today?\n",
            "Tell me your desires.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "  )\n",
        "  #  Max turns exceeded error will be thrown\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's the weather like in Karachi?\",max_turns=1)\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "937yC6ERyQ1M",
        "outputId": "54ff6f8a-5cf8-4b80-be1c-73ae0ce69cc2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MaxTurnsExceeded",
          "evalue": "Max turns (1) exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-51-4045774385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-51-4045774385.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#  Max turns exceeded error will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tool Use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"What's the weather like in Karachi?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         return await runner.run(\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                             ),\n\u001b[1;32m    404\u001b[0m                         )\n\u001b[0;32m--> 405\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mMaxTurnsExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max turns ({max_turns}) exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     logger.debug(\n",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m: Max turns (1) exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\"required\"),\n",
        "      reset_tool_choice=False, # this is prefered now and tool choice reset will bring us into tool calling loop\n",
        "  )        # if max_turns would not have been applied it will continue calling tools and then llm until api rate limit or perhaps default max_turns = 10\n",
        "                                         #  Max turns exceeded error will be thrown\n",
        "  with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"Hi\",max_turns=3)\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "_EEdBIQm0OsE",
        "outputId": "4f872b1e-b276-45ca-d034-a1e0cbe6247d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MaxTurnsExceeded",
          "evalue": "Max turns (3) exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52-774086153.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-52-774086153.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#  Max turns exceeded error will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tool Use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_AGENT_RUNNER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         return await runner.run(\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                             ),\n\u001b[1;32m    404\u001b[0m                         )\n\u001b[0;32m--> 405\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mMaxTurnsExceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max turns ({max_turns}) exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     logger.debug(\n",
            "\u001b[0;31mMaxTurnsExceeded\u001b[0m: Max turns (3) exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather],\n",
        "      model_settings = ModelSettings(temperature=1, tool_choice=\".45.,/j[\"),\n",
        "      reset_tool_choice=False,\n",
        "\n",
        "  )                                              # tool_choice = auto/None is by default and llm will make sure if it needs tool\n",
        "                                                 # tool_choice = required will make sure tool is called no matter the prompt\n",
        "  with trace(\"Tool Use\"):           # tool_choice = \"none\" or empty or any string value will make sure tool is never called\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_8hqNVj2l9j",
        "outputId": "9bc0cead-4cb6-4d09-915a-984307966540"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karachi's weather\n",
            "Sunny skies, warm, gentle breeze\n",
            "A pleasant day calls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parallel Tool Call"
      ],
      "metadata": {
        "id": "B2U3TxJS5fcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "async def main():\n",
        "  @function_tool\n",
        "  async def  get_weather(location: str) -> str:\n",
        "   return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "  @function_tool\n",
        "  async def  get_news(location: str) -> str:\n",
        "   return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok since the whole nb is Async the parallel_tool_calls=False is not stopping parallel tool call\n",
        "  with trace(\"Tool Use\"):            # we will try again in next cell with synchronus stuff\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CGFxZH35jJi",
        "outputId": "01360506-f6cb-4849-8c8d-1275ccf2c38a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karachi's sunny skies,\n",
            "Good news flows from its warm heart,\n",
            "A city thrives on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "@function_tool\n",
        "def  get_weather(location: str) -> str:\n",
        " return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "@function_tool\n",
        "def  get_news(location: str) -> str:\n",
        " return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "async def main():\n",
        "\n",
        "  agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok now since the tools are sync and rest of nb is Async the parallel_tool_calls=False is not stopping parallel tool call\n",
        "  with trace(\"Tool Use\"):           # try again next cell\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTMwPgbS86b1",
        "outputId": "986784fb-3a7b-421f-8e72-b41e5731de2c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather is sunny,\n",
            "Good news from Karachi today,\n",
            "A pleasant bright day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool, model_settings, ModelSettings\n",
        "\n",
        "@function_tool\n",
        "def  get_weather(location: str) -> str:\n",
        " return  f\"The weather in {location} is Sunny\"\n",
        "\n",
        "@function_tool\n",
        "def  get_news(location: str) -> str:\n",
        " return  f\"Positive News are coming from {location}\"\n",
        "\n",
        "agent = Agent(\n",
        "      name = \"Assistant\",\n",
        "      instructions = \"respond in haiku\",\n",
        "      model = model,\n",
        "      tools = [get_weather,get_news],\n",
        "      model_settings = ModelSettings(parallel_tool_calls=False),\n",
        "  )\n",
        "                                       # Ok now since whole nb is Async the parallel_tool_calls=False is still not stopping parallel tool call\n",
        "with trace(\"Tool Use\"):\n",
        "    res = await Runner.run(agent,\"What's the weather in Karachi and what kind of news are coming out of there?\")\n",
        "    print(res.final_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiBr-5MZ9o8B",
        "outputId": "85adfb7d-1ede-461b-f6b2-2c41cabf0f22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Karachi, sun shines,\n",
            "Good news flows from this great city,\n",
            "A bright day unfolds.\n"
          ]
        }
      ]
    }
  ]
}