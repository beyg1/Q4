{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf7mEIDOEdTSiL/aFSJv/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyg1/Q4/blob/main/Openai%20SDK/ToolCallOpenaiAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Tx0hzHzdJ3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9bf962-7c31-413f-9c41-027c82d8d965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.4/100.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() #Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "Pxug1F9Dk7Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True\n",
        "    )"
      ],
      "metadata": {
        "id": "jm-DTLGPlBMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools\n",
        "The OpenAI Agents SDK provides a robust framework for integrating various tools into agents, enabling them to perform tasks such as data retrieval, web searches, and code execution. Here's an overview of the key points regarding tool integration:\n",
        "\n",
        "# Types of Tools:\n",
        "\n",
        "### Hosted Tools: These are pre-built tools running on OpenAI's servers, accessible via the [OpenAIResponsesModel]\n",
        "\n",
        "### WebSearchTool: Enables agents to perform web searches.\n",
        "\n",
        "### FileSearchTool: Allows retrieval of information from OpenAI Vector Stores.\n",
        "\n",
        "### ComputerTool: Facilitates automation of computer-based tasks.\n",
        "\n",
        "### Function Calling: This feature allows agents to utilize any Python function as a tool, enhancing their versatility.\n",
        "\n",
        "### Agents as Tools: Agents can employ other agents as tools, enabling hierarchical task management without transferring control.\n",
        "\n",
        "# Implementing Tools:\n",
        "\n",
        "# Function Tools:\n",
        "By decorating Python functions with @function_tool, they can be seamlessly integrated as tools for agents.\n",
        "\n",
        "# Tool Execution Flow:\n",
        "During an agent's operation, if a tool call is identified in the response, the SDK processes the tool call, appends the tool's response to the message history, and continues the loop until a final output is produced.\n",
        "\n",
        "# Error Handling:\n",
        "The SDK offers mechanisms to handle errors gracefully, allowing agents to recover from tool-related issues and continue their tasks effectively."
      ],
      "metadata": {
        "id": "bw74YzQrlZxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tools"
      ],
      "metadata": {
        "id": "VzOSx2fXtfed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.tool import function_tool\n",
        "@function_tool(\"get_weather\")\n",
        "def get_weather(location: str) -> str:\n",
        "  \"\"\"\n",
        "  Fetch the weather for a given location, returning a short description.\n",
        "  \"\"\"\n",
        "  # Example logic\n",
        "  return f\"The weather in {location} is 22 degrees Celsius.\"\n",
        "\n",
        "@function_tool(\"piaic_student_finder\")\n",
        "def student_finder(student_roll: int) -> str:\n",
        "  \"\"\"\n",
        "  find the PIAIC student based on the roll number\n",
        "  \"\"\"\n",
        "  data = {\n",
        "          1: \"Qasim\",\n",
        "          2: \"Sir Zia\",\n",
        "          3: \"Daniyal\"\n",
        "         }\n",
        "\n",
        "  return data.get(student_roll, \"Not Found\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5MHmiz9ttjuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq tavily-python\n",
        "#tavily search provides agents with internet search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU0eYd6W94pX",
        "outputId": "036adc8c-e97c-42bb-d0eb-ebaa4a615c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.tool import function_tool\n",
        "import os\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Retrieve the Tavily API key from environment variables or userdata\n",
        "tavily_api_key = os.environ.get(\"Tavily_API_Key\") or userdata.get(\"Tavily_API_Key\")\n",
        "if not tavily_api_key:\n",
        "    raise ValueError(\"TAVILY_API_KEY is not set. Please ensure it is defined in your environment variables or userdata.\")\n",
        "\n",
        "# Initialize the Tavily search client\n",
        "tavily_client = TavilyClient(api_key=tavily_api_key)\n",
        "\n",
        "@function_tool(\"internet_search\")\n",
        "def internet_search(query: str) -> str:\n",
        "  \"\"\"\n",
        "  Search the internet for the given query using Tavily and return the result.\n",
        "  \"\"\"\n",
        "  response = tavily_client.search(query)\n",
        "\n",
        "  # Assuming 'response' is a dictionary, extract the relevant information\n",
        "  # and format it into a string.\n",
        "  # You may need to adjust this based on the structure of Tavily's response.\n",
        "  # Example:\n",
        "  # return response.get('results', [])[0].get('content', '')\n",
        "  return str(response) # For now, returning the entire response as a string\n",
        "\n"
      ],
      "metadata": {
        "id": "dwhBY5297H08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        tools=[get_weather, student_finder, internet_search], # add tools here\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(agent, \"can you access the internet search tool and find out who is the president of Pakistan\")\n",
        "    print(result.final_output)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kESxzVZ7uGY8",
        "outputId": "b3f2c1fd-941b-4f55-e17f-c49eb74e3fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asif Ali Zardari\n",
            "Is president now, it is he,\n",
            "Takes the office now.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}